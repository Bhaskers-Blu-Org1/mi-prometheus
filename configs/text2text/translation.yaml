# Problem parameters:
training:
    problem:
        name: &name translation
        batch_size: &b 32
        training_size: &ts 0.90
        output_lang_name: 'fra'
        max_sequence_length: &seq 15
        eng_prefixes: ["i am ", "i m ", "he is", "he s ", "she is", "she s", "you are", "you re ", "we are", "we re ", "they are", "they re "]
        use_train_data: True
        data_folder: '~/data/language'
        reverse: False

    cuda: True

    # set optimizer
    optimizer:
        # Exact name of the pytorch optimizer function
        name: SGD
        # Function arguments of the optimizer, by name
        lr: 0.01

    terminal_condition:
        loss_stop: 0.1
        max_episodes: 1300


# Problem parameters:
validation:
    problem:
        name: *name
        batch_size: *b
        training_size: *ts
        output_lang_name: 'fra'
        max_sequence_length: *seq
        eng_prefixes: ["i am ", "i m ", "he is", "he s ", "she is", "she s", "you are", "you re ", "we are", "we re ", "they are", "they re "]
        use_train_data: True
        data_folder: '~/data/language'
        reverse: False


# Problem parameters:
testing:
    problem:
        name: *name
        batch_size: *b
        training_size: *ts
        output_lang_name: 'fra'
        max_sequence_length: *seq
        eng_prefixes: ["i am ", "i m ", "he is", "he s ", "she is", "she s", "you are", "you re ", "we are", "we re ", "they are", "they re "]
        use_train_data: False
        data_folder: '~/data/language'
        reverse: False



# Model parameters:
model:
    name: simple_encoder_decoder
    max_length: 15
    input_voc_size: 3506  # this value is coming from the problem class (problem.input_lang.n_words)
    hidden_size: 256
    output_voc_size: 5231
    encoder_bidirectional: True
