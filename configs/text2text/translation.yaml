# Problem parameters:
problem_train:
    name: &name translation
    batch_size: &b 64
    training_size: &t 0.90
    output_lang_name: 'fra'
    max_sequence_length: &seq 15
    eng_prefixes: !!python/tuple ["i am ", "i m ", "he is", "he s ", "she is", "she s", "you are", "you re ", "we are", "we re ", "they are", "they re "]
    use_train_data: True
    data_folder: '~/data/language'
    reverse: True
    cuda: True

# Problem parameters:
problem_test:
    name: *name
    batch_size: *b
    training_size: *t
    output_lang_name: 'fra'
    max_sequence_length: *seq
    eng_prefixes: !!python/tuple ["i am ", "i m ", "he is", "he s ", "she is", "she s", "you are", "you re ", "we are", "we re ", "they are", "they re "]
    use_train_data: False
    data_folder: '~/data/language'
    reverse: True

# Problem parameters:
problem_validation:
    name: *name
    batch_size: *b
    training_size: *t
    output_lang_name: 'fra'
    max_sequence_length: *seq
    eng_prefixes: !!python/tuple ["i am ", "i m ", "he is", "he s ", "she is", "she s", "you are", "you re ", "we are", "we re ", "they are", "they re "]
    use_train_data: True
    data_folder: '~/data/language'
    reverse: True

# Model parameters:
model:
    name: simple_encoder_decoder
    max_length: *seq
    input_voc_size: 5244  # this value is coming from the problem class (problem.input_lang.n_words)
    hidden_size: 256
    output_voc_size: 3499

# optimizer parameters:
optimizer:
    # Exact name of the pytorch optimizer function
    name: SGD
    # Function arguments of the optimizer, by name
    lr: 0.01

# settings parameters
settings:
    # masked output and target
    use_mask: False
    length_loss: 10
    loss_stop: 0.1
    max_episodes: 100000
    seed_numpy: 4354
    seed_torch: 2452
