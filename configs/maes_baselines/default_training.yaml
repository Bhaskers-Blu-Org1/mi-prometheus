# Problem parameters:
training:
    # Curriculum learning - optional.
    curriculum_learning:
        interval: 500
        initial_max_sequence_length: 5

    # Optimizer parameters:
    optimizer:
        # Exact name of the pytorch optimizer function
        name: Adam
        # Function arguments of the optimizer, by name
        lr: 0.01

    # Optional parameter, its presence results in clipping gradient to a range (-gradient_clipping, gradient_clipping)
    gradient_clipping: 10
    # How often the model will be validated/saved.
    validation_interval: 100
    # Optional parameters denoting number of - used when validation section is not present.
    length_loss: 10

    # Terminal condition parameters:
    terminal_condition:
        loss_stop: 1.0e-5
        max_episodes: 100000

# Additional settings:
settings:
    cuda: True
    seed_numpy: -1
    seed_torch: -1
