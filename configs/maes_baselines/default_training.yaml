# Problem parameters:
training:
    # Curriculum learning - optional.
    curriculum_learning:
        initial_max_sequence_length: 5

    # Optimizer parameters:
    optimizer:
        # Exact name of the pytorch optimizer function
        name: Adam
        # Function arguments of the optimizer, by name
        lr: 0.01

    # Optional parameter, its presence results in clipping gradient to a range (-gradient_clipping, gradient_clipping)
    gradient_clipping: 10

    # Terminal condition parameters:
    terminal_condition:
        loss_stop: 1.0e-5
        max_episodes: 100
