# Problem parameters:
training:
    problem:
        name: &name CIFAR10
        batch_size: &b 64
        index: [0, 40000]
        use_train_data: True
        root_dir: '~/data/cifar10'
        padding: &p [0,0,0,0] # ex: (x1, x2, x3, x4) pad last dim by (x1, x2) and 2nd to last by (x3, x4)
        up_scaling: &scale True # if up_scale true the image is resized to 224 x 224
    # optimizer parameters:
    optimizer:
        name: Adam
        lr: 0.01
    terminal_conditions:
        loss_stop: 1.0e-5
        episode_limit: 50000

# Problem parameters:
validation:
    problem:
        name: *name
        batch_size: *b
        index: [40000, 49999]
        use_train_data: True # True because we are splitting the training set to: validation and training
        padding: *p
        root_dir: '~/data/cifar10'
        up_scaling: *scale

# Problem parameters:
testing:
    problem:
        name: *name
        batch_size: *b
        use_train_data: False
        padding: *p
        root_dir: '~/data/cifar10'
        up_scaling: *scale

# Model parameters:
model:
    name: AlexnetWrapper
    pretrained: False
