# Problem parameters:
problem_train:
    name: &name cifar
    # Size of generated input: [batch_size x sequence_length x number of control and data bits].
    batch_size: 64
    start_index: 0
    stop_index: 40000
    use_train_data: True
    folder: '~/data/cifar10'
    padding: &p [0,0,0,0] # ex: (x1, x2, x3, x4) pad last dim by (x1, x2) and 2nd to last by (x3, x4)
    cuda: False  # The presence of the 'cuda' key is optional

# Problem parameters:
problem_test:
    name: *name
    # Size of generated input: [batch_size x sequence_length x number of control + data bits].
    batch_size: 64
    start_index: 0
    stop_index: 9999
    use_train_data: False
    padding: *p
    folder: '~/data/cifar10'

# Problem parameters:
problem_validation:
    name: *name
    # batch size
    batch_size: 64
    start_index: 40000
    stop_index: 49999
    use_train_data: True
    padding: *p
    folder: '~/data/cifar10'

# Model parameters:
model:
    name: simple_cnn   # dynamic_working_memory
    # Input bits = [control_bits, data_bits]
    # Output bits = [data_bits]
    depth_conv1: 6
    depth_conv2: 16
    filter_size_conv1: 5
    filter_size_conv2: 5
    num_pooling: 2

    # image size
    num_channels: 3
    height: 32
    width: 32
    padding: *p

# optimizer parameters:
optimizer:
    # Exact name of the pytorch optimizer function
    name: Adam
    # Function arguments of the optimizer, by name
    lr: 0.01

# settings parameters
settings:
    # masked output and target
    use_mask: True
    length_loss: 10
    loss_stop: 1.0e-5
    max_episodes: 50000
    seed_numpy: 4354
    seed_torch: 2452
