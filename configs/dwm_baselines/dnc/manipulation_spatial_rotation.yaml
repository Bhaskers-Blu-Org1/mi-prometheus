#parameters to change
# batch_size 1 - 32
# hidden_state_dim (5 - 64)
# num_reads (1 - 4)
# lr (.0001 to .01)
# memory_content_size ( 10 -20)
# The seeds maybe

# Problem parameters:
problem_train:
    name: &name manipulation_spatial_rotation
    # Size of generated input: [batch_size x sequence_length x number of control + data bits].
    control_bits: &cbits 2
    data_bits: &dbits 8
    batch_size: &bs 16
    # Parameters denoting min and max lengths. 
    min_sequence_length: 1
    max_sequence_length: 10
    bias: 0.5
   #Parameter of curriculum learning, increase the max_sequence_length
    #every ```curriculum_learning_interval```
    #Optional parameter. 0 disables it.
    #curriculum_learning_interval: 0
    curriculum_learning:
        interval: 0
        initial_max_sequence_length: 5

    # Optional parameter, its presence results in clipping gradient to a range (-gradient_clipping, gradient_clipping)
    gradient_clipping: 10
    num_bits: .5

# Problem parameters:
problem_test:
    name: *name
    # Size of generated input: [batch_size x sequence_length x number of control + data bits].
    control_bits: *cbits
    data_bits: *dbits
    batch_size: 64
    # Parameters denoting min and max lengths.
    min_sequence_length: 1000
    max_sequence_length: 1000
    bias: 0.5 
    num_bits: .5

# Problem parameters:
problem_validation:
    name: *name
    # Size of generated input: [batch_size x sequence_length x number of control + data bits].
    control_bits: *cbits
    data_bits: *dbits
    batch_size: 64
    # Parameters denoting min and max lengths.
    min_sequence_length: 100
    max_sequence_length: 100
    bias: 0.5
    num_bits: .5

# Model parameters:
model:
    name: dnc   # differential neural computer
    # Input bits = [control_bits, data_bits]
    # Output bits = [data_bits]
    control_bits: *cbits
    data_bits: *dbits
    batch_size: *bs
    # Controller hidden state.
    hidden_state_dim: 20
    # Memory (?).
    memory_content_size: 10
    memory_addresses_size: -1
    # Number of heads.
    num_writes: 1
    num_reads: 1
    # Addressing parameters.
    use_content_addressing: True
    shift_size: 3
    controller_type: lstm
    use_ntm_write: False
    use_ntm_read: False
    use_ntm_order: False
    use_extra_write_gate: False
    non_linearity: sigmoid
    # active the plotting of the memory and attention
    plot_memory: False

# optimizer parameters:
optimizer:
    # Exact name of the pytorch optimizer function
    #name: RMSprop
    name: Adam
    # Function arguments of the optimizer, by name
    lr: 5.0e-4
    #lr: 1.0e-4
    #eps: 1.0e-10

# settings parameters
settings:
    # masked output and target
    use_mask: True
    length_loss: 10
    loss_stop: 1.0e-4
    max_episodes: 100000
    seed_numpy: 0
    seed_torch: 2
