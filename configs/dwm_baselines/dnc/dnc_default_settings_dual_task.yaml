# Problem parameters:
training: 
    problem:
        # Size of generated input: [batch_size x sequence_length x number of control and data bits].
        batch_size: 64
        # Parameters denoting min and max lengths.
        min_sequence_length: 1
        max_sequence_length: 6
        num_subseq_min: 1
        num_subseq_max: 3
    # Set optimizer.
    optimizer:
        name: Adam
        lr: 0.0005
    # Optional parameter, its presence results in clipping gradient to a range (-gradient_clipping, gradient_clipping)
    gradient_clipping: 10
    # Terminal condition parameters:
    terminal_conditions:
        loss_stop: 0.0001
        eposides_limit: 100000

# Problem parameters:
testing:
    problem:
        # Size of generated input: [batch_size x sequence_length x number of control + data bits].
        batch_size: 64
        # Parameters denoting min and max lengths.
        min_sequence_length: 50
        max_sequence_length: 50
        num_subseq_min: 20
        num_subseq_max: 20

# Problem parameters:
validation:
    problem:
        # Size of generated input: [batch_size x sequence_length x number of control + data bits].
        batch_size: 64
        # Parameters denoting min and max lengths.
        min_sequence_length: 20
        max_sequence_length: 20
        num_subseq_min: 5
        num_subseq_max: 5
