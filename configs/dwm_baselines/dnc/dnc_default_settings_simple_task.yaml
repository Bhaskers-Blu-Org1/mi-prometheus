# Problem parameters:
training: 
    problem:
        # Size of generated input: [batch_size x sequence_length x number of control and data bits].
        batch_size: 16
        # Parameters denoting min and max lengths.
        min_sequence_length: 1
        max_sequence_length: 10
    # Set optimizer.
    optimizer:
        name: Adam
        lr: 0.0005
    # Optional parameter, its presence results in clipping gradient to a range (-gradient_clipping, gradient_clipping)
    gradient_clipping: 10
    # Terminal condition parameters:
    terminal_conditions:
        loss_stop: 0.0001
        eposides_limit: 100000

# Problem parameters:
validation:
    problem:
        # Size of generated input: [batch_size x sequence_length x number of control + data bits].
        batch_size: 64
        # Parameters denoting min and max lengths.
        min_sequence_length: 100
        max_sequence_length: 100

# Problem parameters:
testing:
    problem:
        # Size of generated input: [batch_size x sequence_length x number of control + data bits].
        batch_size: 64
        # Parameters denoting min and max lengths.
        min_sequence_length: 1000
        max_sequence_length: 1000
